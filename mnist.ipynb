{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST CNN Classifier\n",
    "Adapted from the [mnist_cnn keras example](https://github.com/keras-team/keras/blob/master/examples/mnist_cnn.py) pointed to from the [AWS tutorial on containers](https://aws.amazon.com/getting-started/tutorials/train-deep-learning-model-aws-ec2-containers/):\n",
    "```\n",
    "'''Trains a simple convnet on the MNIST dataset.\n",
    "Gets to 99.25% test accuracy after 12 epochs\n",
    "(there is still a lot of margin for parameter tuning).\n",
    "16 seconds per epoch on a GRID K520 GPU.\n",
    "'''\n",
    "\n",
    "from __future__ import print_function\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras import backend as K\n",
    "\n",
    "batch_size = 128\n",
    "num_classes = 10\n",
    "epochs = 12\n",
    "\n",
    "# input image dimensions\n",
    "img_rows, img_cols = 28, 28\n",
    "\n",
    "# the data, split between train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "if K.image_data_format() == 'channels_first':\n",
    "    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n",
    "    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n",
    "    input_shape = (1, img_rows, img_cols)\n",
    "else:\n",
    "    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
    "    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
    "    input_shape = (img_rows, img_cols, 1)\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                 activation='relu',\n",
    "                 input_shape=input_shape))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adadelta(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=(x_test, y_test))\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Further adapted from Yan LeCunn's top performing MNIST paper:\n",
    "[Regularization of Neural Networks using DropConnect](http://yann.lecun.com/exdb/publis/pdf/wan-icml-13.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup libraries and parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import imageio\n",
    "import os.path\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "#from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For confusion matrix and 2D embedding\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.neighbors import NearestNeighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIL\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Run this cell to mount your Google Drive.\n",
    "    data_path = '/content/drive'\n",
    "    from google.colab import drive\n",
    "    drive.mount(data_path)\n",
    "except:\n",
    "    # Just write locally if not in Colaboratory\n",
    "    data_path = './checkpoints'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Control the resolution of figures plotted below. 200 dpi works well on my macbook\n",
    "plt.rcParams['figure.dpi'] = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network definition\n",
    "Adapted from this portion of the Keras example model\n",
    "```\n",
    "model.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                 activation='relu',\n",
    "                 input_shape=input_shape))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "```\n",
    "A few modifications:\n",
    "* I changed the first layer to output 8 channels instead of 32, with the rationale being that a 3x3 kernel only spans 9 DOF. Any channels above 9 would start to become linearly dependent, but the nonlinearity after them might allow them to learn more?\n",
    "* I added batch normalization after the first two convolutional layers because...well...everyone says they are helpful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
    "        # weights = 1x8x3x3 = 72\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "        # weights = 8x64x3x3 = 4,608\n",
    "        self.drop1 = nn.Dropout(0.25)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.fc1 = nn.Linear(12*12*64, 128)\n",
    "        # weights = 12x12x64x128 = 1,179,648\n",
    "        self.drop2 = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "        # weights = 128 x 10 = 1280\n",
    "\n",
    "    def forward(self, x):\n",
    "        # layer size 28x28x1 = 784\n",
    "        cl1 = F.relu(self.bn1(self.conv1(x)))\n",
    "        # layer size 26x26x8 = 5,408\n",
    "        # perf = 21,632 outputs x 288 kernel entries = 6,230,016 mul adds\n",
    "        cl2 = F.relu(self.bn2(self.conv2(cl1)))\n",
    "        # layer size 24x24x64 = 36,864\n",
    "        # perf = 36,864 outputs x 589,824 kernel entries = 21,743,271,936 mul adds\n",
    "        mp = F.max_pool2d(cl2, 2, 2)\n",
    "        # layer size 12x12x64 = 9216\n",
    "        mp_d = self.drop1(mp)\n",
    "        # layer size 1x12*12*64 = 9216\n",
    "        fc1 = F.relu(self.fc1(mp_d.view(-1, 12*12*64)))\n",
    "        # layer size 1x128\n",
    "        # perf = 1,179,648 mul adds\n",
    "        fc2 = self.fc2(self.drop2(fc1))\n",
    "        # layer size 1x10\n",
    "        # perf = 1280 mul adds\n",
    "        return F.log_softmax(fc2, dim=1)\n",
    "    \n",
    "    # Output fc1 features\n",
    "    def features(self, x):\n",
    "        # layer size 28x28x1 = 784\n",
    "        cl1 = F.relu(self.bn1(self.conv1(x)))\n",
    "        # layer size 26x26x8 = 5,408\n",
    "        # perf = 21,632 outputs x 288 kernel entries = 6,230,016 mul adds\n",
    "        cl2 = F.relu(self.bn2(self.conv2(cl1)))\n",
    "        # layer size 24x24x64 = 36,864\n",
    "        # perf = 36,864 outputs x 589,824 kernel entries = 21,743,271,936 mul adds\n",
    "        mp = F.max_pool2d(cl2, 2, 2)\n",
    "        # layer size 12x12x64 = 9216\n",
    "        mp_d = self.drop1(mp)\n",
    "        # layer size 1x12*12*64 = 9216\n",
    "        fc1 = F.relu(self.fc1(mp_d.view(-1, 12*12*64)))\n",
    "        # layer size 1x128\n",
    "        # perf = 1,179,648 mul adds\n",
    "        return fc1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer:\n",
    "    def __init__(self,\n",
    "                 seed=1,\n",
    "                 no_cuda=False,\n",
    "                 #load_checkpoint=12,\n",
    "                 load_checkpoint=False,\n",
    "                 checkpoint_path='.'):\n",
    "        \n",
    "        self.batch_size=120\n",
    "        self.test_batch_size=1000\n",
    "        self.epochs=1000\n",
    "        self.lr=0.01\n",
    "        self.momentum=0.5\n",
    "        self.log_interval=250\n",
    "        \n",
    "        self.checkpoint_path = checkpoint_path\n",
    "        self.save_checkpoints=True\n",
    "\n",
    "        self.epoch = 1\n",
    "        \n",
    "        torch.manual_seed(seed)\n",
    "\n",
    "        use_cuda = not no_cuda and torch.cuda.is_available()\n",
    "        self.kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}\n",
    "\n",
    "        self.device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "        self.model = Net().to(self.device)\n",
    "        \n",
    "        #self.optimizer = optim.SGD(self.model.parameters(), lr=self.lr, momentum=self.momentum)\n",
    "        #self.optimizer = optim.Adadelta(self.model.parameters(), lr=self.lr)\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=1e-4)\n",
    "        self.loss_criterion = nn.CrossEntropyLoss(reduction='sum')\n",
    "\n",
    "        if load_checkpoint:\n",
    "            self.load_checkpoint(load_checkpoint)\n",
    "            self.test(test_loader)\n",
    "\n",
    "    def set_optimizer(self, lr):\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr)\n",
    "        \n",
    "    def train(self, train_loader):\n",
    "        # Set model to be in training mode\n",
    "        self.model.train()\n",
    "        \n",
    "        # Accuracy accumulator \n",
    "        correct = 0\n",
    "        total_loss = 0\n",
    "        mini_batch_losses = []\n",
    "        for data, target in train_loader:\n",
    "            data, target = data.to(self.device), target.to(self.device)\n",
    "            self.optimizer.zero_grad()\n",
    "            output = self.model(data)\n",
    "\n",
    "            pred = output.argmax(dim=1, keepdim=True) # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "            loss = self.loss_criterion(output, target)\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "            mini_batch_losses.append(loss.item())\n",
    "\n",
    "        return total_loss, correct, mini_batch_losses\n",
    "        \n",
    "    def test(self, test_loader):\n",
    "        # Set model to be in testing mode\n",
    "        self.model.eval()\n",
    "\n",
    "        # Accuracy accumulator \n",
    "        correct = 0\n",
    "        total_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for data, target in test_loader:\n",
    "                data, target = data.to(self.device), target.to(self.device)\n",
    "                output = self.model(data)\n",
    "\n",
    "                pred = output.argmax(dim=1, keepdim=True) # get the index of the max log-probability\n",
    "                correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "                total_loss += self.loss_criterion(output, target).item() # sum up batch loss\n",
    "\n",
    "        return total_loss, correct\n",
    "\n",
    "    def run_epoch(self, train_loader, test_loader):\n",
    "        train_start = time.perf_counter()\n",
    "        train_loss, train_correct, mini_batch_losses = self.train(train_loader)\n",
    "        train_end = time.perf_counter()\n",
    "        train_count = len(train_loader.dataset)\n",
    "        train_delta = train_end - train_start\n",
    "\n",
    "        test_start = time.perf_counter()\n",
    "        test_loss, test_correct = self.test(test_loader)\n",
    "        test_end = time.perf_counter()\n",
    "        test_count = len(test_loader.dataset)\n",
    "        test_delta = test_end - test_start\n",
    "\n",
    "        print('{:5d} | {:5.1f}s, {:4.1f}s | {:6.4f}, {:6.4f} | {:5d}/{:5d} ({:.2f}%), {:4d}/{:4d} ({:5.2f}%)'.format(\n",
    "            self.epoch,\n",
    "            train_delta, test_delta,\n",
    "            train_loss/train_count, test_loss/test_count,\n",
    "            train_correct, train_count, 100. * train_correct / train_count,\n",
    "            test_correct, test_count, 100. * test_correct / test_count))\n",
    "        \n",
    "        self.epoch = self.epoch + 1\n",
    "        \n",
    "    def run(self, train_loader, test_loader, epochs=False):\n",
    "        \n",
    "        if not epochs:\n",
    "            epochs = self.epochs\n",
    "\n",
    "        print('Epoch | Time          | Loss           | Accuracy')\n",
    "        print('      | Train,  Test  | Train,  Test   | Train,                Test')\n",
    "        for epoch in range(epochs):\n",
    "            self.run_epoch(train_loader, test_loader)\n",
    "            if self.save_checkpoints:\n",
    "                self.save_checkpoint()\n",
    "    \n",
    "    def save_checkpoint(self, prefix='mnist_'):\n",
    "        checkpoint = {'epoch' : self.epoch,\n",
    "                      'model_state_dict': self.model.state_dict(),\n",
    "                      'optimizer_state_dict' : self.optimizer.state_dict(),\n",
    "                      'loss_criterion' : self.loss_criterion}\n",
    "        torch.save(checkpoint, os.path.join(self.checkpoint_path, '{}{:03}.pth'.format(prefix, self.epoch)))\n",
    "\n",
    "    def load_checkpoint(self, epoch, prefix='mnist_'):\n",
    "        checkpoint = torch.load(os.path.join(self.checkpoint_path, '{}{:03}.pth'.format(prefix, epoch)))\n",
    "        self.model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        self.loss_criterion = checkpoint['loss_criterion']\n",
    "        for parameter in self.model.parameters():\n",
    "            parameter.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = Optimizer(checkpoint_path=data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network Graph Plotting\n",
    "These are the graph of functions back propagated through during the call to backward. This is adapted from the [PyTorch tutorial documentation](https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# depth-first search through the back prop functions\n",
    "def print_graph(fn, depth=0):\n",
    "    if fn is None:\n",
    "        return\n",
    "\n",
    "    print('|'*depth, type(fn).__name__)\n",
    "    for i in range(len(fn.next_functions)):\n",
    "        print_graph(fn.next_functions[i][0], depth+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a random input and run it forward\n",
    "batch_size = 1\n",
    "input = torch.randn(batch_size,1,28,28).to(opt.device)\n",
    "output = opt.model(input)\n",
    "\n",
    "# Run the backward pass, which generates the graph\n",
    "opt.model.zero_grad()\n",
    "output.backward(torch.randn(batch_size,10).to(opt.device))\n",
    "\n",
    "# Print the graph\n",
    "print_graph(output.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print Model Parameter Sizes\n",
    "How many weights/parameters are there in each expression (portion of a layer) that contribute to the gradient?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_model_size(model):\n",
    "    total_model_size = 0\n",
    "    print(\"Model's state_dict:\")\n",
    "    # print out size of each set of parameter\n",
    "    for param_tensor in model.state_dict():\n",
    "        sz = model.state_dict()[param_tensor].size()\n",
    "\n",
    "        # skip empty parameters (like batch normalization history length)\n",
    "        if len(sz) == 0:\n",
    "            continue\n",
    "\n",
    "        total_model_size += np.prod(sz)\n",
    "        if len(sz) > 1:\n",
    "            print('{:16s} {:26s} : {} x {} = {:,}'.format(param_tensor, str(sz), sz[0], np.prod(sz[1:]), np.prod(sz)))\n",
    "        else:\n",
    "            print('{:16s} {:26s} : {:,}'.format(param_tensor, str(sz), np.prod(sz)))\n",
    "\n",
    "    # print out total\n",
    "    print('\\n{:43} : {:,}'.format('Total Model Size',total_model_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    " print_model_size(opt.model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataLoaders|\n",
    "Wrap dataset downloaders in a DataLoader class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataLoaders for train and test data\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data', train=True, download=True,\n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.RandomAffine(10,translate=(0.1,0.1),scale=(0.9,1.1),shear=None,resample=PIL.Image.BILINEAR),\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "    batch_size=opt.batch_size, shuffle=True, **opt.kwargs)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data', train=False, transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "    batch_size=opt.test_batch_size, **opt.kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train (or load) the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt.run(train_loader, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a 2800 x 2800 tiled image from the 10,000 test images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tile_digits(digits, outputs=None, shape='Square', train=False):\n",
    "    ds = datasets.MNIST('../data', train=train, transform=transforms.Compose([\n",
    "                           transforms.ToTensor()]))\n",
    "\n",
    "    cmap = mpl.cm.jet\n",
    "    cmaplist = [cmap(i) for i in range(cmap.N)]\n",
    "    cmap = mpl.colors.LinearSegmentedColormap.from_list('Custom cmap', cmaplist, N=10)\n",
    "    \n",
    "    def get_img(idx):\n",
    "        if idx >= len(digits) or idx < 0 or digits[idx] < -10:\n",
    "            return np.zeros((28,28,3)).astype('uint8')\n",
    "        \n",
    "        idx = digits[idx]\n",
    "        if idx < 0:\n",
    "            clr = cmap(-idx-1)\n",
    "            img = np.full((28,28,1), 255)\n",
    "        else:\n",
    "            clr = cmap(ds[idx][1])\n",
    "            img = ds[idx][0].numpy().reshape(28,28)*255\n",
    "            \n",
    "        return np.dstack([(img*clr[c]).astype('uint8') for c in range(3)])\n",
    "\n",
    "    def get_target(idx):\n",
    "        if idx >= len(digits) or idx < 0 or digits[idx] < 0:\n",
    "            return -1\n",
    "        return ds[digits[idx]][1]\n",
    "\n",
    "    def get_class(idx):\n",
    "        if idx >= len(digits) or idx < 0 or digits[idx] < 0:\n",
    "            return -1\n",
    "        return outputs[digits[idx]].argmax(dim=0, keepdim=True).numpy().T[0]\n",
    "\n",
    "    cnt = len(digits)\n",
    "    if shape is 'Square':\n",
    "        cols = np.ceil(np.sqrt(cnt)).astype('int64')\n",
    "    elif shape is 'Horizontal':\n",
    "        cols = cnt\n",
    "    else:\n",
    "        cols = 1\n",
    "\n",
    "    img = np.vstack([np.hstack([get_img(off+col) for col in range(cols)]) for off in range(0, cnt, cols)])\n",
    "    targets = np.vstack([np.hstack([get_target(off+col) for col in range(cols)]) for off in range(0, cnt, cols)])\n",
    "    if outputs is None:\n",
    "        return img, targets\n",
    "    else:\n",
    "        classes = np.vstack([np.hstack([get_class(off+col) for col in range(cols)]) for off in range(0, cnt, cols)])\n",
    "\n",
    "    return img, targets, classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tiled, target = tile_digits(range(10000))\n",
    "imageio.imwrite('tiled.png', tiled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax=plt.subplots(figsize=(5,5), dpi=200)\n",
    "ax.axis('off')\n",
    "plt.imshow(tiled)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect Failures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_failures(model, device, test_loader):\n",
    "    model.eval()\n",
    "\n",
    "    batch_offset = 0\n",
    "    failure_indices = []\n",
    "    target_all = torch.zeros((0),dtype=torch.long)\n",
    "    output_all = torch.zeros((0,10))\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "\n",
    "            target_all = torch.cat((target_all, target))\n",
    "            output_all = torch.cat((output_all, output))\n",
    "\n",
    "            pred = output.argmax(dim=1, keepdim=True) # get the index of the max log-probability\n",
    "            idx = np.where(pred.ne(target.view_as(pred)))[0]\n",
    "            failure_indices += list(idx + batch_offset)\n",
    "            \n",
    "            batch_offset += pred.size(0)\n",
    "    return np.array(failure_indices), output_all, target_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "failure_indices, output_all, target_all = get_failures(opt.model, opt.device, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "target_failures = target_all[failure_indices]\n",
    "failure_idx_by_digit = list(map(lambda digit: np.where(target_failures.eq(digit))[0], range(0,10)))\n",
    "\n",
    "# Print sorted failures\n",
    "sorted_failures = np.hstack(failure_idx_by_digit)\n",
    "fail_img, targets, classes = tile_digits(failure_indices[sorted_failures], output_all)\n",
    "\n",
    "print(len(failure_indices))\n",
    "print('Classifications:\\n', classes)\n",
    "imageio.imwrite('sorted_failures.png', fail_img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax=plt.subplots(figsize=(5,5), dpi=200)\n",
    "ax.axis('off')\n",
    "plt.imshow(fail_img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot t-SNE embedding of fc1 features\n",
    "First extract the fc1 features for each test image and their ground truth values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fc1_features(model, device, dataloader):\n",
    "    # Set model to be in testing mode\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        x = list(map(lambda x: (model.features(x[0].to(device)), x[1].to(device)), dataloader))\n",
    "        fc1 = torch.cat(list(map(lambda x: x[0], x))).detach().numpy()\n",
    "        target = torch.cat(list(map(lambda x: x[1], x))).detach().numpy()\n",
    "        return fc1, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fc1, target=get_fc1_features(opt.model, opt.device, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embed the 128 dimensional fc1 features in 2D space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_embedded = TSNE(n_components=2).fit_transform(fc1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the 2D embedding, color-coded by the ground truth value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmap = mpl.cm.jet\n",
    "cmaplist = [cmap(i) for i in range(cmap.N)]\n",
    "cmap = mpl.colors.LinearSegmentedColormap.from_list('Custom cmap', cmaplist, N=10)\n",
    "\n",
    "#def onclick(event):\n",
    "#    print('%s click: button=%d, x=%d, y=%d, xdata=%f, ydata=%f' %\n",
    "#          ('double' if event.dblclick else 'single', event.button,\n",
    "#           event.x, event.y, event.xdata, event.ydata))\n",
    "\n",
    "fig, (ax0) = plt.subplots(1,1, figsize=(6,5), dpi=200)\n",
    "\n",
    "#cid = fig.canvas.mpl_connect('button_release_event', onclick)\n",
    "\n",
    "cs=ax0.scatter(X_embedded[:,0], X_embedded[:,1],\n",
    "               norm=mpl.colors.Normalize(vmin=-0.5, vmax=9.5),\n",
    "               s=.5, c=target, cmap=cmap)\n",
    "\n",
    "cb = plt.colorbar(cs, ax=ax0)\n",
    "cb.set_ticks(np.arange(0,10))\n",
    "cb.set_ticklabels(np.arange(0, 10))\n",
    "\n",
    "ax0.set_xticks([])\n",
    "ax0.set_yticks([])\n",
    "\n",
    "\n",
    "#ax[1].set_xticks([])\n",
    "#ax[1].set_yticks([])\n",
    "\n",
    "\n",
    "len(target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How about just 4, 7 and 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst = [4,7,9]\n",
    "fc1_sub=fc1[np.isin(target,lst),:]\n",
    "target_sub=target[np.isin(target,lst)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_embedded_sub = TSNE(n_components=2).fit_transform(fc1_sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax0) = plt.subplots(1,1, figsize=(6,5), dpi=200)\n",
    "\n",
    "cmap = mpl.cm.jet\n",
    "cmaplist = [cmap(i) for i in range(cmap.N)]\n",
    "cmap = mpl.colors.LinearSegmentedColormap.from_list('Custom cmap', cmaplist, N=10)\n",
    "\n",
    "cs=plt.scatter(X_embedded_sub[:,0], X_embedded_sub[:,1], s=.5, c=target_sub, cmap=cmap, norm=mpl.colors.Normalize(vmin=-0.5, vmax=9.5))\n",
    "cb = plt.colorbar(cs)\n",
    "cb.set_ticks(np.arange(0,10))\n",
    "cb.set_ticklabels(np.arange(0, 10))\n",
    "\n",
    "ax0.set_xticks([])\n",
    "ax0.set_yticks([])\n",
    "len(target_sub)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What are the Nearest Neighbors for Failures?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataLoaders for train and test data\n",
    "train_loader_fc1 = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data', train=True, download=True,\n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "    batch_size=opt.batch_size, shuffle=False, **opt.kwargs)\n",
    "\n",
    "train_fc1, train_target=get_fc1_features(opt.model, opt.device, train_loader_fc1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbrs = NearestNeighbors(n_neighbors=16, algorithm='ball_tree').fit(train_fc1)\n",
    "distances, indices = nbrs.kneighbors(fc1[failure_indices[sorted_failures],:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt = len(sorted_failures)\n",
    "\n",
    "failure_classes = output_all[failure_indices[sorted_failures]].argmax(dim=1, keepdim=True) # get the index of the max log-probability\n",
    "\n",
    "failure_img = tile_digits(failure_indices[sorted_failures], shape='Vertical')[0]\n",
    "failure_classification_img = tile_digits((-failure_classes.numpy()-1).transpose()[0], shape='Vertical')[0]\n",
    "neigh_img = np.vstack([tile_digits(indices[idx,:], shape='Horizontal', train=True)[0] for idx in range(cnt)])\n",
    "\n",
    "imageio.imwrite('neighbors_train.png', np.hstack([failure_img, failure_classification_img, neigh_img]))\n",
    "\n",
    "fig, ax=plt.subplots(figsize=(5,5), dpi=200)\n",
    "ax.axis('off')\n",
    "ax.imshow(np.hstack([failure_img, failure_classification_img, neigh_img]))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Confusion Matrix\n",
    "The confusion matrix plotting function was copied from [Yassine Ghouzam's great notebook](https://www.kaggle.com/yassineghouzam/introduction-to-cnn-keras-0-997-top-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.colorbar()\n",
    "    plt.title(title)\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j],\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_all = output_all.argmax(dim=1, keepdim=True) # get the index of the max log-probability\n",
    "            \n",
    "# compute the confusion matrix\n",
    "confusion_mtx = confusion_matrix(target_all, pred_all) \n",
    "# plot the confusion matrix\n",
    "plot_confusion_matrix(confusion_mtx, classes = range(10)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Look at weights/kernels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_kernels(conv_layer):\n",
    "    out_dim = conv_layer.size()[0]\n",
    "    in_dim = conv_layer.size()[1]\n",
    "    kw = conv_layer.size()[2]\n",
    "    kh = conv_layer.size()[3]\n",
    "    #np.linalg.svd()\n",
    "\n",
    "    for row in range(0,out_dim):\n",
    "        fig, ax = plt.subplots(1, in_dim)\n",
    "        for col in range(0,in_dim):\n",
    "            if in_dim>1:\n",
    "                col_ax = ax[col]\n",
    "            else:\n",
    "                col_ax = ax\n",
    "            col_ax.imshow(conv_layer[row][col].numpy(), cmap='binary')\n",
    "            col_ax.axis('off')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_kernels(opt.model.state_dict()['conv1.weight'].permute(1,0,2,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_kernels(opt.model.state_dict()['conv2.weight'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
