{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "mnist.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "toc_visible": true
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "1GLE3LqD0aeN"
      },
      "source": [
        "# MNIST CNN Classifier\n",
        "Adapted from the [mnist_cnn keras example](https://github.com/keras-team/keras/blob/master/examples/mnist_cnn.py) pointed to from the [AWS tutorial on containers](https://aws.amazon.com/getting-started/tutorials/train-deep-learning-model-aws-ec2-containers/):\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "lneUeuMr0aeR"
      },
      "source": [
        "Further adapted from Yan LeCunn's top performing MNIST paper:\n",
        "[Regularization of Neural Networks using DropConnect](http://yann.lecun.com/exdb/publis/pdf/wan-icml-13.pdf)\n",
        "\n",
        "https://github.com/j05t/mnist"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "914zXJCc0aeV"
      },
      "source": [
        "# Setup libraries and parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-GEbpdi80aeY",
        "colab": {}
      },
      "source": [
        "import time\n",
        "import pickle\n",
        "import imageio\n",
        "import os.path\n",
        "import numpy as np\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "zip0uHXz0aee",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import curve\n",
        "from torchvision import datasets, transforms\n",
        "#from torch.autograd import Variable"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Db9Uccz_0aek",
        "colab": {}
      },
      "source": [
        "# For confusion matrix and 2D embedding\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.neighbors import NearestNeighbors"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "nvcr8qfq0aen",
        "colab": {}
      },
      "source": [
        "import PIL\n",
        "import itertools"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "O9hbVzMH0aeq",
        "outputId": "6774633f-b4df-45d8-d011-2bcf9526d3e3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        }
      },
      "source": [
        "try:\n",
        "    # Run this cell to mount your Google Drive.\n",
        "    data_path = '/content/drive'\n",
        "    from google.colab import drive\n",
        "    drive.mount(data_path)\n",
        "    data_path = os.path.join(data_path, 'My Drive')\n",
        "except:\n",
        "    # Just write locally if not in Colaboratory\n",
        "    data_path = '.'\n",
        "\n",
        "data_path = os.path.join(data_path, 'checkpoints')\n",
        "try:\n",
        "    !mkdir -p \"$data_path\"\n",
        "except:\n",
        "    print('Couldn\\'t create', data_path)\n",
        "\n",
        "print(data_path)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "/content/drive/My Drive/checkpoints\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bkkbq1TzMCxk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mkdir -p \"$data_path\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "dl37ApE70aeu",
        "colab": {}
      },
      "source": [
        "# Control the resolution of figures plotted below. 200 dpi works well on my macbook\n",
        "plt.rcParams['figure.dpi'] = 200"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "h8o-Gur10ae0"
      },
      "source": [
        "# Network definition\n",
        "Adapted from this portion of the Keras example model\n",
        "```\n",
        "model.add(Conv2D(32, kernel_size=(3, 3),\n",
        "                 activation='relu',\n",
        "                 input_shape=input_shape))\n",
        "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(num_classes, activation='softmax'))\n",
        "```\n",
        "A few modifications:\n",
        "* I changed the first layer to output 8 channels instead of 32, with the rationale being that a 3x3 kernel only spans 9 DOF. Any channels above 9 would start to become linearly dependent, but the nonlinearity after them might allow them to learn more?\n",
        "* I added batch normalization after the first two convolutional layers because...well...everyone says they are helpful"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2XTK5EWLbqM-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ConvLayer(nn.Module):\n",
        "    def __init__(self, ci, co, kernel_size=3, max_pool=False, branches=1):\n",
        "        super(ConvLayer, self).__init__()\n",
        "\n",
        "        co_b = co*branches\n",
        "        # Skip initial 1x1 convolution if single channel input\n",
        "        if ci is 1:\n",
        "            self.model = nn.ModuleList([\n",
        "                nn.Conv2d(1, co_b, (kernel_size,1))\n",
        "            ])\n",
        "        else:\n",
        "            self.model = nn.ModuleList([\n",
        "                nn.Conv2d(ci, co_b, 1, groups=branches),\n",
        "                nn.Conv2d(co_b, co_b, (kernel_size,1), groups=co_b),\n",
        "            ])\n",
        "        self.model.append(nn.BatchNorm2d(co_b))\n",
        "        self.model.append(nn.Conv2d(co_b, co_b, (1,kernel_size), groups=co_b))\n",
        "        self.model.append(nn.LeakyReLU(negative_slope=0.3))\n",
        "        if max_pool:\n",
        "            self.model.append(nn.MaxPool2d(2,2))\n",
        "        self.model.append(nn.BatchNorm2d(co_b))\n",
        "\n",
        "    def forward(self, x):\n",
        "        for m in self.model:\n",
        "            x = m(x)\n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "hS8Da-fM0ae1",
        "colab": {}
      },
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        \n",
        "        # 1 channel input\n",
        "        self.ci = 1\n",
        "        \n",
        "        # channel counts for intermediate layers\n",
        "        self.c1 = 32\n",
        "        self.c2 = 64\n",
        "        self.c3 = 32\n",
        "        self.b = 4\n",
        "        \n",
        "        # 10 channel output\n",
        "        self.co = 10\n",
        "\n",
        "        self.conv_layers = nn.Sequential(\n",
        "            # input: 28 x 28 x 1\n",
        "            ConvLayer(self.ci, self.c1, branches=self.b),\n",
        "            # layer 1: 26 x 26 x 32\n",
        "            ConvLayer(self.c1*self.b, self.c1, branches=self.b),\n",
        "            # layer 2: 24 x 24 x 32\n",
        "    \n",
        "            ConvLayer(self.c1*self.b, self.c2, branches=self.b, max_pool=True),\n",
        "            # layer 3: 11 x 11 x 32\n",
        "            ConvLayer(self.c2*self.b, self.c2, branches=self.b),\n",
        "            # layer 4: 9 x 9 x 32\n",
        "            ConvLayer(self.c2*self.b, self.c2, branches=self.b),                         \n",
        "            # layer 5: 7 x 7 x 32\n",
        "            ConvLayer(self.c2*self.b, self.c2, branches=self.b),\n",
        "            # layer 6: 5 x 5 x 32\n",
        "            ConvLayer(self.c2*self.b, self.c2, branches=self.b),                         \n",
        "            # layer 7: 3 x 3 x 32\n",
        "            ConvLayer(self.c2*self.b, self.c3, branches=self.b),\n",
        "            # layer 8: 1 x 1 x 32\n",
        "        )\n",
        "        \n",
        "        # Second fully connectd layer\n",
        "        self.fc2 = nn.Sequential(\n",
        "            nn.Linear(self.c3*self.b, self.co),\n",
        "            nn.LogSoftmax(dim=1)\n",
        "            # layer 9: 10\n",
        "        )\n",
        "\n",
        "    # Output fc1 features\n",
        "    def features(self, x):\n",
        "        return self.conv_layers.forward(x).view(-1,self.c3*self.b)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        return self.fc2.forward(self.features(x))\n",
        "\n",
        "    def print_size(self):\n",
        "        total_model_size = 0\n",
        "        print(\"Model's state_dict:\")\n",
        "        # print out size of each set of parameter\n",
        "        for param_tensor in self.state_dict():\n",
        "            sz = self.state_dict()[param_tensor].size()\n",
        "\n",
        "            # skip empty parameters (like batch normalization history length)\n",
        "            if len(sz) == 0:\n",
        "                continue\n",
        "\n",
        "            total_model_size += np.prod(sz)\n",
        "            if len(sz) > 1:\n",
        "                print('{:16s} {:26s} : {} x {} = {:,}'.format(param_tensor, str(sz), sz[0], np.prod(sz[1:]), np.prod(sz)))\n",
        "            else:\n",
        "                print('{:16s} {:26s} : {:,}'.format(param_tensor, str(sz), np.prod(sz)))\n",
        "\n",
        "        # print out total\n",
        "        print('\\n{:43} : {:,}'.format('Total Model Size',total_model_size))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "HqTFqMyP0ae3",
        "colab": {}
      },
      "source": [
        "class Optimizer:\n",
        "    def __init__(self,\n",
        "                 seed=1,\n",
        "                 no_cuda=False,\n",
        "                 #load_checkpoint=12,\n",
        "                 load_checkpoint=False,\n",
        "                 checkpoint_path='.',\n",
        "                 checkpoint_prefix='mnist_'):\n",
        "\n",
        "        self.seed = seed\n",
        "        self.batch_size=512\n",
        "        self.test_batch_size=1000\n",
        "        \n",
        "        torch.manual_seed(self.seed)\n",
        "        self.epoch = 0\n",
        "        \n",
        "        use_cuda = not no_cuda and torch.cuda.is_available()\n",
        "        self.kwargs = {'num_workers': 4, 'pin_memory': True} if use_cuda else {}\n",
        "        self.device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "        self.model = Net().to(self.device)\n",
        "        \n",
        "        #self.lr=0.01\n",
        "        #self.momentum=0.5\n",
        "        #self.optimizer = optim.SGD(self.model.parameters(), lr=self.lr, momentum=self.momentum)\n",
        "        #self.optimizer = optim.Adadelta(self.model.parameters(), lr=self.lr)\n",
        "        self.optimizer = curve.Curve(self.model.parameters())\n",
        "        #self.optimizer = optim.Adam(self.model.parameters())\n",
        "        self.line_search_scales = []\n",
        "\n",
        "        self.lr = self.optimizer.defaults['lr']\n",
        "        self.loss_criterion = nn.CrossEntropyLoss(reduction='sum')\n",
        "\n",
        "        self.checkpoint_path = checkpoint_path\n",
        "        self.checkpoint_prefix = checkpoint_prefix\n",
        "        self.save_checkpoints = True\n",
        "        self.checkpoint_interval = 1\n",
        "        \n",
        "        \n",
        "\n",
        "        # reset stats file\n",
        "        if self.save_checkpoints:\n",
        "            fn = os.path.join(self.checkpoint_path, '{}stats.pkl'.format(self.checkpoint_prefix))\n",
        "            if os.path.exists(fn):\n",
        "                os.remove(fn)\n",
        "            \n",
        "        if load_checkpoint:\n",
        "            self.load_checkpoint(load_checkpoint)\n",
        "            self.test(test_loader)\n",
        "            \n",
        "        # DataLoaders for train and test data\n",
        "        self.train_loader = torch.utils.data.DataLoader(\n",
        "            datasets.MNIST('../data', train=True, download=True,\n",
        "                           transform=transforms.Compose([\n",
        "                               transforms.RandomAffine(15,translate=(0.1,0.1),scale=(0.9,1.1),shear=0.3,resample=PIL.Image.BILINEAR),\n",
        "                               transforms.ToTensor(),\n",
        "                               transforms.Normalize((0.1307,), (0.3081,))\n",
        "                           ])),\n",
        "            batch_size=self.batch_size, shuffle=True, **self.kwargs)\n",
        "\n",
        "        self.test_loader = torch.utils.data.DataLoader(\n",
        "            datasets.MNIST('../data', train=False, transform=transforms.Compose([\n",
        "                               transforms.ToTensor(),\n",
        "                               transforms.Normalize((0.1307,), (0.3081,))\n",
        "                           ])),\n",
        "            batch_size=self.test_batch_size, **self.kwargs)\n",
        "\n",
        "    def set_line_search_scales(self, scales):\n",
        "        self.line_search_scales = scales\n",
        "\n",
        "    def set_learning_rate(self, lr):\n",
        "        self.lr = lr\n",
        "        #self.optimizer = optim.Adam(self.model.parameters(), lr)\n",
        "        self.optimizer = curve.Curve(self.model.parameters(), lr)\n",
        "        \n",
        "    def eval_loss(self, step_factor, data, target):\n",
        "        self.model.eval()\n",
        "        self.optimizer.step(eval_only=True, step_factor=step_factor)\n",
        "        output = self.model(data)\n",
        "        self.optimizer.restore_state()\n",
        "        return self.loss_criterion(output, target).item()\n",
        "               \n",
        "    def train(self):\n",
        "        # Set model to be in training mode\n",
        "        self.model.train()\n",
        "        scales = self.line_search_scales\n",
        "        \n",
        "        # Accuracy accumulator \n",
        "        correct = 0\n",
        "        total_loss = 0\n",
        "        mini_batch_losses = []\n",
        "        for data, target in self.train_loader:\n",
        "            data, target = data.to(self.device), target.to(self.device)\n",
        "            self.model.train()\n",
        "\n",
        "            self.optimizer.zero_grad()\n",
        "            output = self.model(data)\n",
        "\n",
        "            pred = output.argmax(dim=1, keepdim=True) # get the index of the max log-probability\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "            loss = self.loss_criterion(output, target)\n",
        "            loss.backward()\n",
        "            total_loss += loss.item()\n",
        "            \n",
        "            # evaluate losses along step direction\n",
        "            if len(scales) > 0:\n",
        "                self.optimizer.save_state()\n",
        "                batch_losses = [self.eval_loss(s, data, target) for s in scales]\n",
        "            else:\n",
        "                batch_losses = [loss.item()]\n",
        "                \n",
        "            # save average losses\n",
        "            mini_batch_losses.append([l/len(data) for l in batch_losses])\n",
        "\n",
        "            # compute and save step factor 1.0\n",
        "            self.optimizer.step() # actually take this step\n",
        "\n",
        "        return total_loss, correct, mini_batch_losses\n",
        "        \n",
        "    def test(self):\n",
        "        # Set model to be in testing mode\n",
        "        self.model.eval()\n",
        "\n",
        "        # Accuracy accumulator \n",
        "        correct = 0\n",
        "        total_loss = 0\n",
        "        with torch.no_grad():\n",
        "            for data, target in self.test_loader:\n",
        "                data, target = data.to(self.device), target.to(self.device)\n",
        "                output = self.model(data)\n",
        "\n",
        "                pred = output.argmax(dim=1, keepdim=True) # get the index of the max log-probability\n",
        "                correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "                total_loss += self.loss_criterion(output, target).item() # sum up batch loss\n",
        "\n",
        "        return total_loss, correct\n",
        "\n",
        "    def run_epoch(self, endl=''):\n",
        "        train_start = time.perf_counter()\n",
        "        train_loss, train_correct, mini_batch_losses = self.train()\n",
        "        train_end = time.perf_counter()\n",
        "        train_count = len(self.train_loader.dataset)\n",
        "        train_delta = train_end - train_start\n",
        "\n",
        "        test_start = time.perf_counter()\n",
        "        test_loss, test_correct = self.test()\n",
        "        test_end = time.perf_counter()\n",
        "        test_count = len(self.test_loader.dataset)\n",
        "        test_delta = test_end - test_start\n",
        "\n",
        "        print('\\r {:1.1e} | {:5d} | {:5.1f}s, {:4.1f}s | {:6.4f}, {:6.4f} | {:5d}/{:5d} ({:.2f}%), {:4d}/{:4d} ({:5.2f}%)'.format(\n",
        "            self.lr,\n",
        "            self.epoch,\n",
        "            train_delta, test_delta,\n",
        "            train_loss/train_count, test_loss/test_count,\n",
        "            train_correct, train_count, 100. * train_correct / train_count,\n",
        "            test_correct, test_count, 100. * test_correct / test_count),\n",
        "            end=endl, flush=True)\n",
        "        self.epoch = self.epoch + 1\n",
        "        \n",
        "        if self.save_checkpoints:\n",
        "            stats = {'losses' : mini_batch_losses,\n",
        "                     'train_loss' : train_loss,\n",
        "                     'train_count' : train_count,\n",
        "                     'train_correct' : train_correct,\n",
        "                     'test_loss' : test_loss,\n",
        "                     'test_count' : test_count,\n",
        "                     'test_correct' : test_correct}\n",
        "            \n",
        "            with open(os.path.join(self.checkpoint_path, '{}stats.pkl'.format(self.checkpoint_prefix)), 'ab') as f:\n",
        "                pickle.dump(stats, f)\n",
        "\n",
        "            if self.epoch % self.checkpoint_interval == 0:\n",
        "                self.save_checkpoint()\n",
        "        \n",
        "    def run(self, epochs=None, lr=None, line_search_scales=None):\n",
        "        if epochs is None:\n",
        "            epochs = self.epochs\n",
        "        if lr is not None:\n",
        "            self.set_learning_rate(lr)\n",
        "        if line_search_scales is not None:\n",
        "            self.set_line_search_scales(line_search_scales)\n",
        "\n",
        "        endl = '\\n'\n",
        "        for epoch in range(epochs):\n",
        "            self.run_epoch(endl=endl)\n",
        "        if endl is '':\n",
        "            print('')            \n",
        "    \n",
        "    def run_schedule(self, schedule):\n",
        "        print('Running schedule ' + '-'.join([str(b['epochs']) for b in schedule]))\n",
        "        print('Learning | Epoch | Time          | Loss           | Accuracy')\n",
        "        print('Rate     |       | Train,  Test  | Train,  Test   | Train,                Test')\n",
        "\n",
        "        np.random.seed(self.seed)\n",
        "        for sched in schedule:\n",
        "            self.run(**sched)\n",
        "            \n",
        "    def save_checkpoint(self):\n",
        "        checkpoint = {'epoch' : self.epoch,\n",
        "                      'model_state_dict': self.model.state_dict(),\n",
        "                      'optimizer_state_dict' : self.optimizer.state_dict(),\n",
        "                      'loss_criterion' : self.loss_criterion}\n",
        "        torch.save(checkpoint, os.path.join(self.checkpoint_path, '{}{:03}.pth'.format(self.checkpoint_prefix, self.epoch)))\n",
        "\n",
        "    def load_checkpoint(self, epoch):\n",
        "        checkpoint = torch.load(os.path.join(self.checkpoint_path, '{}{:03}.pth'.format(self.checkpoint_prefix, epoch)))\n",
        "        self.model.load_state_dict(checkpoint['model_state_dict'])\n",
        "        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "        self.loss_criterion = checkpoint['loss_criterion']\n",
        "        for parameter in self.model.parameters():\n",
        "            parameter.requires_grad = False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jwcOUNzUwfyS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_failures(opt):\n",
        "    opt.model.eval()\n",
        "\n",
        "    batch_offset = 0\n",
        "    failure_indices = []\n",
        "    target_all = torch.zeros((0),dtype=torch.long).to(opt.device)\n",
        "    output_all = torch.zeros((0,10)).to(opt.device)\n",
        "    with torch.no_grad():\n",
        "        for data, target in opt.test_loader:\n",
        "            data, target = data.to(opt.device), target.to(opt.device)\n",
        "            output = opt.model(data)\n",
        "\n",
        "            target_all = torch.cat((target_all, target))\n",
        "            output_all = torch.cat((output_all, output))\n",
        "\n",
        "            pred = output.argmax(dim=1, keepdim=True) # get the index of the max log-probability\n",
        "            idx = np.where(pred.ne(target.view_as(pred)).to(torch.device(\"cpu\")))[0]\n",
        "            failure_indices += list(idx + batch_offset)\n",
        "\n",
        "            batch_offset += pred.size(0)\n",
        "    return np.array(failure_indices), output_all, target_all\n",
        "\n",
        "def get_fc1_features(opt, dataloader, use_final_layer=False):\n",
        "    # Set model to be in testing mode\n",
        "    opt.model.eval()\n",
        "    with torch.no_grad():\n",
        "        targets = torch.cat([target for data, target in dataloader])\n",
        "        # Uses penultimate layer by default, but we can use the 10-d softmax\n",
        "        # output if use_final_layer is True\n",
        "        if use_final_layer:\n",
        "            fc1 = torch.cat([opt.model.forward(data.to(opt.device)) for data, target in dataloader])\n",
        "        else:\n",
        "            fc1 = torch.cat([opt.model.features(data.to(opt.device)) for data, target in dataloader])\n",
        "\n",
        "        targets = targets.to(torch.device(\"cpu\")).detach().numpy()\n",
        "        fc1 = fc1.to(torch.device(\"cpu\")).detach().numpy()\n",
        "\n",
        "        return fc1, targets"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "reDVFlzA0afC"
      },
      "source": [
        "## Network Graph Plotting\n",
        "These are the graph of functions back propagated through during the call to backward. This is adapted from the [PyTorch tutorial documentation](https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "AEbHHvZ-0afD",
        "colab": {}
      },
      "source": [
        "# depth-first search through the back prop functions\n",
        "def print_graph(fn, depth=0):\n",
        "    if fn is None:\n",
        "        return\n",
        "\n",
        "    print('|'*depth, type(fn).__name__)\n",
        "    for i in range(len(fn.next_functions)):\n",
        "        print_graph(fn.next_functions[i][0], depth+1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Nwd6Nnx00afH",
        "outputId": "4f4120f2-6875-43e0-eedf-e8aa2627b961",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "opt = Optimizer()\n",
        "\n",
        "# Generate a random input and run it forward\n",
        "batch_size = 2\n",
        "input = torch.randn(batch_size,1,28,28).to(opt.device)\n",
        "output = opt.model(input)\n",
        "\n",
        "# Run the backward pass, which generates the graph\n",
        "opt.model.zero_grad()\n",
        "output.backward(torch.randn(batch_size,10).to(opt.device))\n",
        "\n",
        "# Print the graph\n",
        "print_graph(output.grad_fn)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " LogSoftmaxBackward\n",
            "| AddmmBackward\n",
            "|| AccumulateGrad\n",
            "|| ViewBackward\n",
            "||| CudnnBatchNormBackward\n",
            "|||| LeakyReluBackward0\n",
            "||||| ThnnConvDepthwise2DBackward\n",
            "|||||| CudnnBatchNormBackward\n",
            "||||||| ThnnConvDepthwise2DBackward\n",
            "|||||||| CudnnConvolutionBackward\n",
            "||||||||| CudnnBatchNormBackward\n",
            "|||||||||| LeakyReluBackward0\n",
            "||||||||||| ThnnConvDepthwise2DBackward\n",
            "|||||||||||| CudnnBatchNormBackward\n",
            "||||||||||||| ThnnConvDepthwise2DBackward\n",
            "|||||||||||||| CudnnConvolutionBackward\n",
            "||||||||||||||| CudnnBatchNormBackward\n",
            "|||||||||||||||| LeakyReluBackward0\n",
            "||||||||||||||||| ThnnConvDepthwise2DBackward\n",
            "|||||||||||||||||| CudnnBatchNormBackward\n",
            "||||||||||||||||||| ThnnConvDepthwise2DBackward\n",
            "|||||||||||||||||||| CudnnConvolutionBackward\n",
            "||||||||||||||||||||| CudnnBatchNormBackward\n",
            "|||||||||||||||||||||| LeakyReluBackward0\n",
            "||||||||||||||||||||||| ThnnConvDepthwise2DBackward\n",
            "|||||||||||||||||||||||| CudnnBatchNormBackward\n",
            "||||||||||||||||||||||||| ThnnConvDepthwise2DBackward\n",
            "|||||||||||||||||||||||||| CudnnConvolutionBackward\n",
            "||||||||||||||||||||||||||| CudnnBatchNormBackward\n",
            "|||||||||||||||||||||||||||| LeakyReluBackward0\n",
            "||||||||||||||||||||||||||||| ThnnConvDepthwise2DBackward\n",
            "|||||||||||||||||||||||||||||| CudnnBatchNormBackward\n",
            "||||||||||||||||||||||||||||||| ThnnConvDepthwise2DBackward\n",
            "|||||||||||||||||||||||||||||||| CudnnConvolutionBackward\n",
            "||||||||||||||||||||||||||||||||| CudnnBatchNormBackward\n",
            "|||||||||||||||||||||||||||||||||| MaxPool2DWithIndicesBackward\n",
            "||||||||||||||||||||||||||||||||||| LeakyReluBackward0\n",
            "|||||||||||||||||||||||||||||||||||| ThnnConvDepthwise2DBackward\n",
            "||||||||||||||||||||||||||||||||||||| CudnnBatchNormBackward\n",
            "|||||||||||||||||||||||||||||||||||||| ThnnConvDepthwise2DBackward\n",
            "||||||||||||||||||||||||||||||||||||||| CudnnConvolutionBackward\n",
            "|||||||||||||||||||||||||||||||||||||||| CudnnBatchNormBackward\n",
            "||||||||||||||||||||||||||||||||||||||||| LeakyReluBackward0\n",
            "|||||||||||||||||||||||||||||||||||||||||| ThnnConvDepthwise2DBackward\n",
            "||||||||||||||||||||||||||||||||||||||||||| CudnnBatchNormBackward\n",
            "|||||||||||||||||||||||||||||||||||||||||||| ThnnConvDepthwise2DBackward\n",
            "||||||||||||||||||||||||||||||||||||||||||||| CudnnConvolutionBackward\n",
            "|||||||||||||||||||||||||||||||||||||||||||||| CudnnBatchNormBackward\n",
            "||||||||||||||||||||||||||||||||||||||||||||||| LeakyReluBackward0\n",
            "|||||||||||||||||||||||||||||||||||||||||||||||| ThnnConvDepthwise2DBackward\n",
            "||||||||||||||||||||||||||||||||||||||||||||||||| CudnnBatchNormBackward\n",
            "|||||||||||||||||||||||||||||||||||||||||||||||||| CudnnConvolutionBackward\n",
            "||||||||||||||||||||||||||||||||||||||||||||||||||| AccumulateGrad\n",
            "||||||||||||||||||||||||||||||||||||||||||||||||||| AccumulateGrad\n",
            "|||||||||||||||||||||||||||||||||||||||||||||||||| AccumulateGrad\n",
            "|||||||||||||||||||||||||||||||||||||||||||||||||| AccumulateGrad\n",
            "||||||||||||||||||||||||||||||||||||||||||||||||| AccumulateGrad\n",
            "||||||||||||||||||||||||||||||||||||||||||||||||| AccumulateGrad\n",
            "||||||||||||||||||||||||||||||||||||||||||||||| AccumulateGrad\n",
            "||||||||||||||||||||||||||||||||||||||||||||||| AccumulateGrad\n",
            "|||||||||||||||||||||||||||||||||||||||||||||| AccumulateGrad\n",
            "|||||||||||||||||||||||||||||||||||||||||||||| AccumulateGrad\n",
            "||||||||||||||||||||||||||||||||||||||||||||| AccumulateGrad\n",
            "||||||||||||||||||||||||||||||||||||||||||||| AccumulateGrad\n",
            "|||||||||||||||||||||||||||||||||||||||||||| AccumulateGrad\n",
            "|||||||||||||||||||||||||||||||||||||||||||| AccumulateGrad\n",
            "||||||||||||||||||||||||||||||||||||||||||| AccumulateGrad\n",
            "||||||||||||||||||||||||||||||||||||||||||| AccumulateGrad\n",
            "||||||||||||||||||||||||||||||||||||||||| AccumulateGrad\n",
            "||||||||||||||||||||||||||||||||||||||||| AccumulateGrad\n",
            "|||||||||||||||||||||||||||||||||||||||| AccumulateGrad\n",
            "|||||||||||||||||||||||||||||||||||||||| AccumulateGrad\n",
            "||||||||||||||||||||||||||||||||||||||| AccumulateGrad\n",
            "||||||||||||||||||||||||||||||||||||||| AccumulateGrad\n",
            "|||||||||||||||||||||||||||||||||||||| AccumulateGrad\n",
            "|||||||||||||||||||||||||||||||||||||| AccumulateGrad\n",
            "||||||||||||||||||||||||||||||||||||| AccumulateGrad\n",
            "||||||||||||||||||||||||||||||||||||| AccumulateGrad\n",
            "|||||||||||||||||||||||||||||||||| AccumulateGrad\n",
            "|||||||||||||||||||||||||||||||||| AccumulateGrad\n",
            "||||||||||||||||||||||||||||||||| AccumulateGrad\n",
            "||||||||||||||||||||||||||||||||| AccumulateGrad\n",
            "|||||||||||||||||||||||||||||||| AccumulateGrad\n",
            "|||||||||||||||||||||||||||||||| AccumulateGrad\n",
            "||||||||||||||||||||||||||||||| AccumulateGrad\n",
            "||||||||||||||||||||||||||||||| AccumulateGrad\n",
            "|||||||||||||||||||||||||||||| AccumulateGrad\n",
            "|||||||||||||||||||||||||||||| AccumulateGrad\n",
            "|||||||||||||||||||||||||||| AccumulateGrad\n",
            "|||||||||||||||||||||||||||| AccumulateGrad\n",
            "||||||||||||||||||||||||||| AccumulateGrad\n",
            "||||||||||||||||||||||||||| AccumulateGrad\n",
            "|||||||||||||||||||||||||| AccumulateGrad\n",
            "|||||||||||||||||||||||||| AccumulateGrad\n",
            "||||||||||||||||||||||||| AccumulateGrad\n",
            "||||||||||||||||||||||||| AccumulateGrad\n",
            "|||||||||||||||||||||||| AccumulateGrad\n",
            "|||||||||||||||||||||||| AccumulateGrad\n",
            "|||||||||||||||||||||| AccumulateGrad\n",
            "|||||||||||||||||||||| AccumulateGrad\n",
            "||||||||||||||||||||| AccumulateGrad\n",
            "||||||||||||||||||||| AccumulateGrad\n",
            "|||||||||||||||||||| AccumulateGrad\n",
            "|||||||||||||||||||| AccumulateGrad\n",
            "||||||||||||||||||| AccumulateGrad\n",
            "||||||||||||||||||| AccumulateGrad\n",
            "|||||||||||||||||| AccumulateGrad\n",
            "|||||||||||||||||| AccumulateGrad\n",
            "|||||||||||||||| AccumulateGrad\n",
            "|||||||||||||||| AccumulateGrad\n",
            "||||||||||||||| AccumulateGrad\n",
            "||||||||||||||| AccumulateGrad\n",
            "|||||||||||||| AccumulateGrad\n",
            "|||||||||||||| AccumulateGrad\n",
            "||||||||||||| AccumulateGrad\n",
            "||||||||||||| AccumulateGrad\n",
            "|||||||||||| AccumulateGrad\n",
            "|||||||||||| AccumulateGrad\n",
            "|||||||||| AccumulateGrad\n",
            "|||||||||| AccumulateGrad\n",
            "||||||||| AccumulateGrad\n",
            "||||||||| AccumulateGrad\n",
            "|||||||| AccumulateGrad\n",
            "|||||||| AccumulateGrad\n",
            "||||||| AccumulateGrad\n",
            "||||||| AccumulateGrad\n",
            "|||||| AccumulateGrad\n",
            "|||||| AccumulateGrad\n",
            "|||| AccumulateGrad\n",
            "|||| AccumulateGrad\n",
            "|| TBackward\n",
            "||| AccumulateGrad\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "LxrZIw1G0afI"
      },
      "source": [
        "## Print Model Parameter Sizes\n",
        "How many weights/parameters are there in each expression (portion of a layer) that contribute to the gradient?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "OKoYjpi70afK",
        "outputId": "80796a2c-d88f-4306-aedb-7d4cfcbfd2df",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "opt.model.print_size()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model's state_dict:\n",
            "conv_layers.0.model.0.weight torch.Size([128, 1, 3, 1]) : 128 x 3 = 384\n",
            "conv_layers.0.model.0.bias torch.Size([128])          : 128\n",
            "conv_layers.0.model.1.weight torch.Size([128])          : 128\n",
            "conv_layers.0.model.1.bias torch.Size([128])          : 128\n",
            "conv_layers.0.model.1.running_mean torch.Size([128])          : 128\n",
            "conv_layers.0.model.1.running_var torch.Size([128])          : 128\n",
            "conv_layers.0.model.2.weight torch.Size([128, 1, 1, 3]) : 128 x 3 = 384\n",
            "conv_layers.0.model.2.bias torch.Size([128])          : 128\n",
            "conv_layers.0.model.4.weight torch.Size([128])          : 128\n",
            "conv_layers.0.model.4.bias torch.Size([128])          : 128\n",
            "conv_layers.0.model.4.running_mean torch.Size([128])          : 128\n",
            "conv_layers.0.model.4.running_var torch.Size([128])          : 128\n",
            "conv_layers.1.model.0.weight torch.Size([128, 32, 1, 1]) : 128 x 32 = 4,096\n",
            "conv_layers.1.model.0.bias torch.Size([128])          : 128\n",
            "conv_layers.1.model.1.weight torch.Size([128, 1, 3, 1]) : 128 x 3 = 384\n",
            "conv_layers.1.model.1.bias torch.Size([128])          : 128\n",
            "conv_layers.1.model.2.weight torch.Size([128])          : 128\n",
            "conv_layers.1.model.2.bias torch.Size([128])          : 128\n",
            "conv_layers.1.model.2.running_mean torch.Size([128])          : 128\n",
            "conv_layers.1.model.2.running_var torch.Size([128])          : 128\n",
            "conv_layers.1.model.3.weight torch.Size([128, 1, 1, 3]) : 128 x 3 = 384\n",
            "conv_layers.1.model.3.bias torch.Size([128])          : 128\n",
            "conv_layers.1.model.5.weight torch.Size([128])          : 128\n",
            "conv_layers.1.model.5.bias torch.Size([128])          : 128\n",
            "conv_layers.1.model.5.running_mean torch.Size([128])          : 128\n",
            "conv_layers.1.model.5.running_var torch.Size([128])          : 128\n",
            "conv_layers.2.model.0.weight torch.Size([256, 32, 1, 1]) : 256 x 32 = 8,192\n",
            "conv_layers.2.model.0.bias torch.Size([256])          : 256\n",
            "conv_layers.2.model.1.weight torch.Size([256, 1, 3, 1]) : 256 x 3 = 768\n",
            "conv_layers.2.model.1.bias torch.Size([256])          : 256\n",
            "conv_layers.2.model.2.weight torch.Size([256])          : 256\n",
            "conv_layers.2.model.2.bias torch.Size([256])          : 256\n",
            "conv_layers.2.model.2.running_mean torch.Size([256])          : 256\n",
            "conv_layers.2.model.2.running_var torch.Size([256])          : 256\n",
            "conv_layers.2.model.3.weight torch.Size([256, 1, 1, 3]) : 256 x 3 = 768\n",
            "conv_layers.2.model.3.bias torch.Size([256])          : 256\n",
            "conv_layers.2.model.6.weight torch.Size([256])          : 256\n",
            "conv_layers.2.model.6.bias torch.Size([256])          : 256\n",
            "conv_layers.2.model.6.running_mean torch.Size([256])          : 256\n",
            "conv_layers.2.model.6.running_var torch.Size([256])          : 256\n",
            "conv_layers.3.model.0.weight torch.Size([256, 64, 1, 1]) : 256 x 64 = 16,384\n",
            "conv_layers.3.model.0.bias torch.Size([256])          : 256\n",
            "conv_layers.3.model.1.weight torch.Size([256, 1, 3, 1]) : 256 x 3 = 768\n",
            "conv_layers.3.model.1.bias torch.Size([256])          : 256\n",
            "conv_layers.3.model.2.weight torch.Size([256])          : 256\n",
            "conv_layers.3.model.2.bias torch.Size([256])          : 256\n",
            "conv_layers.3.model.2.running_mean torch.Size([256])          : 256\n",
            "conv_layers.3.model.2.running_var torch.Size([256])          : 256\n",
            "conv_layers.3.model.3.weight torch.Size([256, 1, 1, 3]) : 256 x 3 = 768\n",
            "conv_layers.3.model.3.bias torch.Size([256])          : 256\n",
            "conv_layers.3.model.5.weight torch.Size([256])          : 256\n",
            "conv_layers.3.model.5.bias torch.Size([256])          : 256\n",
            "conv_layers.3.model.5.running_mean torch.Size([256])          : 256\n",
            "conv_layers.3.model.5.running_var torch.Size([256])          : 256\n",
            "conv_layers.4.model.0.weight torch.Size([256, 64, 1, 1]) : 256 x 64 = 16,384\n",
            "conv_layers.4.model.0.bias torch.Size([256])          : 256\n",
            "conv_layers.4.model.1.weight torch.Size([256, 1, 3, 1]) : 256 x 3 = 768\n",
            "conv_layers.4.model.1.bias torch.Size([256])          : 256\n",
            "conv_layers.4.model.2.weight torch.Size([256])          : 256\n",
            "conv_layers.4.model.2.bias torch.Size([256])          : 256\n",
            "conv_layers.4.model.2.running_mean torch.Size([256])          : 256\n",
            "conv_layers.4.model.2.running_var torch.Size([256])          : 256\n",
            "conv_layers.4.model.3.weight torch.Size([256, 1, 1, 3]) : 256 x 3 = 768\n",
            "conv_layers.4.model.3.bias torch.Size([256])          : 256\n",
            "conv_layers.4.model.5.weight torch.Size([256])          : 256\n",
            "conv_layers.4.model.5.bias torch.Size([256])          : 256\n",
            "conv_layers.4.model.5.running_mean torch.Size([256])          : 256\n",
            "conv_layers.4.model.5.running_var torch.Size([256])          : 256\n",
            "conv_layers.5.model.0.weight torch.Size([256, 64, 1, 1]) : 256 x 64 = 16,384\n",
            "conv_layers.5.model.0.bias torch.Size([256])          : 256\n",
            "conv_layers.5.model.1.weight torch.Size([256, 1, 3, 1]) : 256 x 3 = 768\n",
            "conv_layers.5.model.1.bias torch.Size([256])          : 256\n",
            "conv_layers.5.model.2.weight torch.Size([256])          : 256\n",
            "conv_layers.5.model.2.bias torch.Size([256])          : 256\n",
            "conv_layers.5.model.2.running_mean torch.Size([256])          : 256\n",
            "conv_layers.5.model.2.running_var torch.Size([256])          : 256\n",
            "conv_layers.5.model.3.weight torch.Size([256, 1, 1, 3]) : 256 x 3 = 768\n",
            "conv_layers.5.model.3.bias torch.Size([256])          : 256\n",
            "conv_layers.5.model.5.weight torch.Size([256])          : 256\n",
            "conv_layers.5.model.5.bias torch.Size([256])          : 256\n",
            "conv_layers.5.model.5.running_mean torch.Size([256])          : 256\n",
            "conv_layers.5.model.5.running_var torch.Size([256])          : 256\n",
            "conv_layers.6.model.0.weight torch.Size([256, 64, 1, 1]) : 256 x 64 = 16,384\n",
            "conv_layers.6.model.0.bias torch.Size([256])          : 256\n",
            "conv_layers.6.model.1.weight torch.Size([256, 1, 3, 1]) : 256 x 3 = 768\n",
            "conv_layers.6.model.1.bias torch.Size([256])          : 256\n",
            "conv_layers.6.model.2.weight torch.Size([256])          : 256\n",
            "conv_layers.6.model.2.bias torch.Size([256])          : 256\n",
            "conv_layers.6.model.2.running_mean torch.Size([256])          : 256\n",
            "conv_layers.6.model.2.running_var torch.Size([256])          : 256\n",
            "conv_layers.6.model.3.weight torch.Size([256, 1, 1, 3]) : 256 x 3 = 768\n",
            "conv_layers.6.model.3.bias torch.Size([256])          : 256\n",
            "conv_layers.6.model.5.weight torch.Size([256])          : 256\n",
            "conv_layers.6.model.5.bias torch.Size([256])          : 256\n",
            "conv_layers.6.model.5.running_mean torch.Size([256])          : 256\n",
            "conv_layers.6.model.5.running_var torch.Size([256])          : 256\n",
            "conv_layers.7.model.0.weight torch.Size([128, 64, 1, 1]) : 128 x 64 = 8,192\n",
            "conv_layers.7.model.0.bias torch.Size([128])          : 128\n",
            "conv_layers.7.model.1.weight torch.Size([128, 1, 3, 1]) : 128 x 3 = 384\n",
            "conv_layers.7.model.1.bias torch.Size([128])          : 128\n",
            "conv_layers.7.model.2.weight torch.Size([128])          : 128\n",
            "conv_layers.7.model.2.bias torch.Size([128])          : 128\n",
            "conv_layers.7.model.2.running_mean torch.Size([128])          : 128\n",
            "conv_layers.7.model.2.running_var torch.Size([128])          : 128\n",
            "conv_layers.7.model.3.weight torch.Size([128, 1, 1, 3]) : 128 x 3 = 384\n",
            "conv_layers.7.model.3.bias torch.Size([128])          : 128\n",
            "conv_layers.7.model.5.weight torch.Size([128])          : 128\n",
            "conv_layers.7.model.5.bias torch.Size([128])          : 128\n",
            "conv_layers.7.model.5.running_mean torch.Size([128])          : 128\n",
            "conv_layers.7.model.5.running_var torch.Size([128])          : 128\n",
            "fc2.0.weight     torch.Size([10, 128])      : 10 x 128 = 1,280\n",
            "fc2.0.bias       torch.Size([10])           : 10\n",
            "\n",
            "Total Model Size                            : 115,466\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "4ScxfTzL0afO"
      },
      "source": [
        "# Train (or load) the network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2QWZWDiEAMB_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8ff95401-0a5c-48ab-f5f5-588a4b06060e"
      },
      "source": [
        "scales = [-1] + [2**x for x in range(-4,3,1)]\n",
        "scales = []\n",
        "schedules = [\n",
        "    [\n",
        "        {\"epochs\" : 50, \"lr\" : 1e-3, \"line_search_scales\" : []},\n",
        "        {\"epochs\" : 25, \"lr\" : 1e-4, \"line_search_scales\" : []},\n",
        "        {\"epochs\" : 25, \"lr\" : 1e-5, \"line_search_scales\" : []},\n",
        "#        {\"epochs\" : 13, \"lr\" : 1e-3, \"line_search_scales\" : scales},\n",
        "#        {\"epochs\" : 3, \"lr\" : 1e-4, \"line_search_scales\" : scales},\n",
        "    ],\n",
        "]\n",
        "print(scales)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W0xSCtVyeAXE",
        "colab_type": "code",
        "outputId": "b9c8b138-7033-4f4e-bea7-279473c25a9a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "seed = 1\n",
        "optimizers = []\n",
        "for idx, sched in enumerate(schedules):\n",
        "    opt = Optimizer(checkpoint_path=data_path,\n",
        "                                checkpoint_prefix='opt_{}_'.format(idx),\n",
        "                                seed=seed,\n",
        "                                load_checkpoint=False)\n",
        "    optimizers.append(opt)\n",
        "    opt.run_schedule(sched)\n",
        "    np.random.seed(seed)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Running schedule 50-25-25\n",
            "Learning | Epoch | Time          | Loss           | Accuracy\n",
            "Rate     |       | Train,  Test  | Train,  Test   | Train,                Test\n",
            " 1.0e-03 |     0 |  25.8s,  1.9s | 0.9641, 0.1282 | 44193/60000 (73.66%), 9739/10000 (97.39%)\n",
            " 1.0e-03 |     1 |  25.6s,  1.9s | 0.1744, 0.0800 | 57455/60000 (95.76%), 9798/10000 (97.98%)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZYFs_RKQrobS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(optimizers)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yv5PZrHMV1ta",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "schedules = [\n",
        "    [\n",
        "        {\"epochs\" : 25, \"lr\" : 1e-6, \"line_search_scales\" : []},\n",
        "    ],\n",
        "]\n",
        "for opt, sched in zip(optimizers, schedules):\n",
        "    opt.run_schedule(sched)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0fo3vmkPCWGX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_stats(optimizers, stat_slc):\n",
        "    train_error=[]\n",
        "    test_error=[]\n",
        "    train_loss=[]\n",
        "    test_loss=[]\n",
        "    train_losses=[]\n",
        "    for opt in optimizers:\n",
        "        with open(os.path.join(opt.checkpoint_path, '{}stats.pkl'.format(opt.checkpoint_prefix)), 'rb') as f:\n",
        "            stats = []\n",
        "            while 1:\n",
        "                try:\n",
        "                    stats.append(pickle.load(f))\n",
        "                except EOFError:\n",
        "                    break\n",
        "        test_error.append(np.hstack([(s['test_count']-s['test_correct'])/s['test_count'] for s in stats]))\n",
        "        train_error.append(np.hstack([(s['train_count']-s['train_correct'])/s['train_count'] for s in stats]))\n",
        "        test_loss.append(np.hstack([s['test_loss']/s['test_count'] for s in stats]))\n",
        "        train_loss.append(np.hstack([s['train_loss']/s['train_count'] for s in stats]))\n",
        "        train_losses.append(np.vstack([np.vstack(s['losses']) for s in stats[stat_slc]]))\n",
        "    return test_error, train_error, test_loss, train_loss, train_losses"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1rgTQxDLPach",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "opt_slc = slice(0,len(schedules),1)\n",
        "test_error, train_error, test_loss, train_loss, train_losses0 = get_stats(optimizers[opt_slc], slice(0,3,1))\n",
        "#_, _, _, _, train_losses1 = get_stats(optimizers[opt_slc], slice(10, 13, 1))\n",
        "_, _, _, _, train_losses2 = get_stats(optimizers[opt_slc], slice(3, 16, 1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7dpLsEASMCNe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def quadratic_fit(scales, losses):\n",
        "    quadratics = [np.polyfit(scales, losses[idx,:], 2) for idx in range(losses.shape[0])]\n",
        "    return [-q[1]/q[0] for q in quadratics]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gEHfQgE7bdDr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.hist(quadratic_fit(optimizers[0].line_search_scales, train_losses0[0]), bins='auto')\n",
        "plt.xlim(-5,5)\n",
        "plt.title('Epochs 0-2 (lr = 1e-3)')\n",
        "plt.xlabel('Optimal Step Size Prediction')\n",
        "plt.ylabel('Count')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6zEnlTXZbem8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.hist(quadratic_fit(optimizers[0].line_search_scales, train_losses2[0]), bins=200)\n",
        "plt.xlim(-50,50)\n",
        "plt.title('Epochs 3-16 (lr = 1e-4)')\n",
        "plt.xlabel('Optimal Step Size Prediction')\n",
        "plt.ylabel('Count')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wXBSG_LgxdF4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "scales = optimizers[0].line_search_scales\n",
        "best_indices = np.argmin(train_losses0[0],1)\n",
        "best_scales = [scales[idx] for idx in best_indices]\n",
        "plt.hist(best_scales, bins=200)\n",
        "plt.title('Epochs 0-2 (lr = 1e-3)')\n",
        "plt.xlabel('Lowest Loss Scale Factor')\n",
        "plt.ylabel('Count')\n",
        "plt.xlim(-2,5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hes-AAhC1RbC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "scales = optimizers[0].line_search_scales\n",
        "best_indices = np.argmin(train_losses2[0],1)\n",
        "best_scales = [scales[idx] for idx in best_indices]\n",
        "plt.hist(best_scales, bins=200)\n",
        "plt.title('Epochs 3-16 (lr = 1e-4)')\n",
        "plt.xlabel('Lowest Loss Scale Factor')\n",
        "plt.ylabel('Count')\n",
        "plt.xlim(-2,5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VfDhOUjFPZWk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cmap=plt.get_cmap('coolwarm')\n",
        "scales = optimizers[0].line_search_scales\n",
        "batch_size = len(optimizers[0].train_loader)\n",
        "offset = 0\n",
        "scl_slc = (0,5)\n",
        "#for batch, l in enumerate([train_losses0[0], train_losses1[0], train_losses2[0]]):\n",
        "for batch, l in enumerate([train_losses0[0], train_losses2[0]]):\n",
        "    x_rng = np.arange(0, l.shape[0])/batch_size + offset\n",
        "    handles = plt.plot(x_rng, l[:,scl_slc], '-')\n",
        "    for idx, h in enumerate(handles):\n",
        "        h.set_color(cmap(idx/len(scl_slc)))\n",
        "        h.set_linewidth(0.25)\n",
        "        h.set_markersize(0.25)\n",
        "        if batch is 1:\n",
        "            h.set_label(scales[scl_slc[idx]])\n",
        "    offset = x_rng[-1]\n",
        "\n",
        "plt.title('Losses')\n",
        "plt.ylabel('Cross-Entropy (nits)')\n",
        "plt.xlabel('Epoch')\n",
        "plt.yscale('log')\n",
        "plt.legend()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vNJJ5CQgrdU7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cmap=plt.get_cmap('coolwarm')\n",
        "scales = optimizers[0].line_search_scales\n",
        "batch_size = len(optimizers[0].train_loader)\n",
        "offset = 0\n",
        "#for batch, l in enumerate([train_losses0[0], train_losses1[0], train_losses2[0]]):\n",
        "for batch, l in enumerate([train_losses0[0], train_losses2[0]]):\n",
        "    x_rng = np.arange(0, l.shape[0])/batch_size + offset\n",
        "    offset_idx = np.where( np.hstack(scales) == 1.0)[0][0]\n",
        "    #handles = plt.plot(x_rng, l-np.tile(l[:,offset_idx], (len(scales),1)).transpose())\n",
        "    handles = plt.plot(x_rng, l[:,0]-l[:,offset_idx].transpose())\n",
        "    for idx, h in enumerate(handles):\n",
        "        h.set_color(cmap(idx/(offset_idx*2+1)))\n",
        "        h.set_linewidth(0.25)\n",
        "        if batch is 1:\n",
        "            h.set_label(scales[idx])\n",
        "    offset = x_rng[-1]\n",
        "\n",
        "plt.title('Losses Relative to Nominal Step (1.0)')\n",
        "plt.ylabel('Difference in Cross-Entropy (nits)')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EUbvcv5Xkis_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cmap=plt.get_cmap('coolwarm')\n",
        "scales = optimizers[0].line_search_scales\n",
        "batch_size = len(optimizers[0].train_loader)\n",
        "offset = train_losses0[0].shape[0]/batch_size\n",
        "#for batch, l in enumerate([train_losses0[0], train_losses1[0], train_losses2[0]]):\n",
        "for batch, l in enumerate([train_losses2[0]]):\n",
        "    x_rng = np.arange(0, l.shape[0])/batch_size + offset\n",
        "    handles = plt.plot(x_rng, l-np.tile(l[:,offset_idx], (len(scales),1)).transpose(), '-')\n",
        "    for idx, h in enumerate(handles):\n",
        "        h.set_color(cmap(idx/(offset_idx*2+1)))\n",
        "        h.set_linewidth(0.25)\n",
        "        h.set_markersize(0.5)\n",
        "        if batch is 0:\n",
        "            h.set_label(scales[idx])\n",
        "    offset = x_rng[-1]\n",
        "\n",
        "plt.title('Losses Relative to Nominal Step (1.0)')\n",
        "plt.ylabel('Difference in Cross-Entropy (nits)')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dpCxlSKMDRyB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_losses_and_errors(schedules, train_error, test_error, train_loss, test_loss, train_losses):\n",
        "    train_cmap = plt.get_cmap('Blues')\n",
        "    test_cmap = plt.get_cmap('Reds')\n",
        "\n",
        "    # Create labels for each schedule (e.g. '100-50-100')\n",
        "    lbls=[]\n",
        "    for s in schedules:\n",
        "        epochs = [str(b['epochs']) for b in s]\n",
        "        lbls.append('-'.join(epochs))\n",
        "\n",
        "    schcnt = len(lbls)\n",
        "\n",
        "    def set_handles(handles, label, cmap):\n",
        "        linestyles = ['solid', 'dashed', 'dashdot', (0, (3, 5, 1, 5, 1, 5)), (0, (1, 5)), 'dotted']\n",
        "        markers = ['o', '+', 'v', 'D', 'p', 'o']\n",
        "\n",
        "        for h, lbl, clr in zip(handles, lbls, np.arange(0,1,1/schcnt)*0.4+0.6):\n",
        "            h.set_linestyle('-')\n",
        "            h.set_color(cmap(clr))\n",
        "            h.set_marker('')\n",
        "            h.set_markersize(2)\n",
        "            h.set_label(label + ' ' + lbl)\n",
        "            h.set_linewidth(1)\n",
        "\n",
        "    show_train = True\n",
        "    show_test = True\n",
        "\n",
        "    fig, ax = plt.subplots(2,1, figsize=(10,12), dpi=100)\n",
        "\n",
        "    # Plot Losses\n",
        "    plt.subplot(211)\n",
        "\n",
        "    epochs = len(train_error[0])\n",
        "    x_coords = np.arange(0, epochs)+1\n",
        "    x_ticks = np.arange(0, epochs, step=10)\n",
        "\n",
        "    if show_train:\n",
        "        handles = plt.plot(x_coords, np.vstack(train_loss).transpose())\n",
        "        set_handles(handles, 'Training (Avg)', train_cmap)\n",
        "\n",
        "    if show_test:\n",
        "        handles = plt.plot(x_coords, np.vstack(test_loss).transpose())\n",
        "        set_handles(handles, 'Test (Avg)', test_cmap)\n",
        "\n",
        "    plt.legend()\n",
        "    plt.xticks(x_ticks)\n",
        "    plt.xlim(1, epochs, 10)\n",
        "    plt.yscale('log')\n",
        "    plt.ylim(1e-3, 1e-1)\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Cross-Entropy (nits)')\n",
        "    plt.title('Losses')\n",
        "    plt.grid(which='both')\n",
        "\n",
        "    # Plot Error Rates\n",
        "    plt.subplot(212)\n",
        "\n",
        "    if show_train:\n",
        "        handles = plt.plot(x_coords, np.vstack(train_error).transpose()*100)\n",
        "        set_handles(handles, 'Training', train_cmap)\n",
        "\n",
        "    if show_test:\n",
        "        handles = plt.plot(x_coords, np.vstack(test_error).transpose()*100)\n",
        "        set_handles(handles, 'Test', test_cmap)\n",
        "\n",
        "    plt.legend()\n",
        "    plt.xticks(x_ticks)\n",
        "    plt.xlim(1, epochs)\n",
        "    ymax = 1\n",
        "    plt.ylim(0, ymax)\n",
        "    plt.yticks(np.arange(0, ymax, step=ymax/10))\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Error Rate (%)')\n",
        "    plt.title('Error Rates')\n",
        "    plt.grid(which='both')\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CPus9U29TcMo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plot_losses_and_errors(schedules, train_error, test_error, train_loss, test_loss, train_losses0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "I0-SOg8s0afQ"
      },
      "source": [
        "# Visualization Code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "PCRKD4-o0afR"
      },
      "source": [
        "## Create a 2800 x 2800 tiled image from the 10,000 test images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Ajyf9s0w0afS",
        "colab": {}
      },
      "source": [
        "def tile_digits(digits, outputs=None, shape='Square', train=False):\n",
        "    ds = datasets.MNIST('../data', train=train, transform=transforms.Compose([\n",
        "                           transforms.ToTensor()]))\n",
        "\n",
        "    cmap = mpl.cm.jet\n",
        "    cmaplist = [cmap(i) for i in range(cmap.N)]\n",
        "    cmap = mpl.colors.LinearSegmentedColormap.from_list('Custom cmap', cmaplist, N=10)\n",
        "    \n",
        "    def get_img(idx):\n",
        "        if idx >= len(digits) or idx < 0 or digits[idx] < -10:\n",
        "            return np.zeros((28,28,3)).astype('uint8')\n",
        "        \n",
        "        idx = digits[idx]\n",
        "        if idx < 0:\n",
        "            clr = cmap(-idx-1)\n",
        "            img = np.full((28,28,1), 255)\n",
        "        else:\n",
        "            clr = cmap(ds[idx][1])\n",
        "            img = ds[idx][0].numpy().reshape(28,28)*255\n",
        "            \n",
        "        return np.dstack([(img*clr[c]).astype('uint8') for c in range(3)])\n",
        "\n",
        "    def get_target(idx):\n",
        "        if idx >= len(digits) or idx < 0 or digits[idx] < 0:\n",
        "            return -1\n",
        "        return ds[digits[idx]][1]\n",
        "\n",
        "    def get_class(idx):\n",
        "        if idx >= len(digits) or idx < 0 or digits[idx] < 0:\n",
        "            return -1\n",
        "        return outputs[digits[idx]].argmax(dim=0, keepdim=True).numpy().T[0]\n",
        "\n",
        "    cnt = len(digits)\n",
        "    if shape is 'Square':\n",
        "        cols = np.ceil(np.sqrt(cnt)).astype('int64')\n",
        "    elif shape is 'Horizontal':\n",
        "        cols = cnt\n",
        "    else:\n",
        "        cols = 1\n",
        "\n",
        "    img = np.vstack([np.hstack([get_img(off+col) for col in range(cols)]) for off in range(0, cnt, cols)])\n",
        "    targets = np.vstack([np.hstack([get_target(off+col) for col in range(cols)]) for off in range(0, cnt, cols)])\n",
        "    if outputs is None:\n",
        "        return img, targets\n",
        "    else:\n",
        "        classes = np.vstack([np.hstack([get_class(off+col) for col in range(cols)]) for off in range(0, cnt, cols)])\n",
        "\n",
        "    return img, targets, classes"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "fevtqtgo0afT",
        "colab": {}
      },
      "source": [
        "tiled, target = tile_digits(range(10000))\n",
        "imageio.imwrite(os.path.join(data_path, 'tiled.png'), tiled)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2pp3ID-00afW",
        "colab": {}
      },
      "source": [
        "fig, ax=plt.subplots(figsize=(5,5), dpi=200)\n",
        "ax.axis('off')\n",
        "plt.imshow(tiled)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "fFqiLBir0afY"
      },
      "source": [
        "## Inspect Failures"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Z8gbwV8f0afa",
        "colab": {}
      },
      "source": [
        "failure_indices, output_all, target_all = get_failures(optimizers[-1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "BXQ9Au-A0afb",
        "scrolled": true,
        "colab": {}
      },
      "source": [
        "target_failures = target_all[failure_indices]\n",
        "failure_idx_by_digit = list(map(lambda digit: np.where(target_failures.eq(digit).to(torch.device(\"cpu\")))[0], range(0,10)))\n",
        "\n",
        "# Print sorted failures\n",
        "sorted_failures = np.hstack(failure_idx_by_digit)\n",
        "fail_img, targets, classes = tile_digits(failure_indices[sorted_failures], output_all.to(torch.device(\"cpu\")))\n",
        "\n",
        "print(len(failure_indices))\n",
        "print('Classifications:\\n', classes)\n",
        "imageio.imwrite(os.path.join(data_path, 'sorted_failures.png'), fail_img)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "WYYtuSQt0afc",
        "colab": {}
      },
      "source": [
        "fig, ax=plt.subplots(figsize=(5,5), dpi=200)\n",
        "ax.axis('off')\n",
        "plt.imshow(fail_img)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "4eWPYEc-0afd"
      },
      "source": [
        "## Plot t-SNE embedding of fc1 features\n",
        "First extract the fc1 features for each test image and their ground truth values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "eiaKKFhe0afg",
        "colab": {}
      },
      "source": [
        "opt = optimizers[-1]\n",
        "epochs = optimizers[-1].epoch\n",
        "fc1 = []\n",
        "target = []\n",
        "epochs_list = range(epochs-3, epochs)\n",
        "for epoch in epochs_list:\n",
        "    optimizers[-1].load_checkpoint(epoch+1)\n",
        "    f, t = get_fc1_features(opt, opt.test_loader, use_final_layer=False)\n",
        "    fc1.append(f)\n",
        "    target.append(t)\n",
        "    \n",
        "fc1 = np.vstack(fc1)\n",
        "target = np.hstack(target)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Apc9jXVO0afi"
      },
      "source": [
        "Embed the 10 dimensional softmax output in 2D space"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "DMKQ6FI40afj",
        "colab": {}
      },
      "source": [
        "tsne = TSNE(n_components=2, verbose=2)\n",
        "X_embedded=tsne.fit_transform(fc1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WHgoPNvf-f-A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tsne.__dict__"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "KlIRQhBE0afk"
      },
      "source": [
        "Plot the 2D embedding, color-coded by the ground truth value"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Uw0OUpDv0afk",
        "colab": {}
      },
      "source": [
        "cmap = mpl.cm.jet\n",
        "cmaplist = [cmap(i) for i in range(cmap.N)]\n",
        "cmap = mpl.colors.LinearSegmentedColormap.from_list('Custom cmap', cmaplist, N=10)\n",
        "\n",
        "#def onclick(event):\n",
        "#    print('%s click: button=%d, x=%d, y=%d, xdata=%f, ydata=%f' %\n",
        "#          ('double' if event.dblclick else 'single', event.button,\n",
        "#           event.x, event.y, event.xdata, event.ydata))\n",
        "\n",
        "fig, ax = plt.subplots(len(epochs_list), 1, figsize=(6,len(epochs_list)*5), dpi=200)\n",
        "\n",
        "#cid = fig.canvas.mpl_connect('button_release_event', onclick)\n",
        "for i, epoch in enumerate(epochs_list):\n",
        "    test_count = len(opt.test_loader.dataset)\n",
        "    idx = range(test_count*i, test_count*(i+1))\n",
        "    cs=ax[i].scatter(X_embedded[idx,0], X_embedded[idx,1],\n",
        "                   norm=mpl.colors.Normalize(vmin=-0.5, vmax=9.5),\n",
        "                   s=.5, c=target[idx], cmap=cmap)\n",
        "\n",
        "    cb = plt.colorbar(cs, ax=ax[i])\n",
        "    cb.set_ticks(np.arange(0,10))\n",
        "    cb.set_ticklabels(np.arange(0, 10))\n",
        "\n",
        "    ax[i].set_xticks([])\n",
        "    ax[i].set_yticks([])\n",
        "\n",
        "\n",
        "    #ax[1].set_xticks([])\n",
        "    #ax[1].set_yticks([])\n",
        "\n",
        "\n",
        "len(target)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "E4FF8LU60afl"
      },
      "source": [
        "### How about just 4, 7 and 9"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xehv-4ri0afl",
        "colab": {}
      },
      "source": [
        "lst = [4,7,9]\n",
        "fc1_sub=fc1[np.isin(target,lst),:]\n",
        "target_sub=target[np.isin(target,lst)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "t9DGu2CW0afp",
        "colab": {}
      },
      "source": [
        "X_embedded_sub = TSNE(n_components=2, verbose=2).fit_transform(fc1_sub)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "FDs6KH8v0afq",
        "colab": {}
      },
      "source": [
        "fig, ax = plt.subplots(len(epochs_list),1, figsize=(6,5*len(epochs_list)), dpi=200)\n",
        "\n",
        "cmap = mpl.cm.jet\n",
        "cmaplist = [cmap(i) for i in range(cmap.N)]\n",
        "cmap = mpl.colors.LinearSegmentedColormap.from_list('Custom cmap', cmaplist, N=10)\n",
        "\n",
        "for i, epoch in enumerate(epochs_list):\n",
        "    test_count = len(target_sub)//len(epochs_list)\n",
        "    idx = range(test_count*i, test_count*(i+1))\n",
        "\n",
        "    cs=ax[i].scatter(X_embedded_sub[idx,0], X_embedded_sub[idx,1],\n",
        "                     s=.5, c=target_sub[idx], cmap=cmap,\n",
        "                     norm=mpl.colors.Normalize(vmin=-0.5, vmax=9.5))\n",
        "    cb = plt.colorbar(cs, ax=ax[i])\n",
        "    cb.set_ticks(np.arange(0,10))\n",
        "    cb.set_ticklabels(np.arange(0, 10))\n",
        "\n",
        "    ax[i].set_xticks([])\n",
        "    ax[i].set_yticks([])\n",
        "    \n",
        "len(target_sub)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "xPOAH86q0afr"
      },
      "source": [
        "## What are the Nearest Neighbors for Failures?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "gTtRxRa-0afr",
        "colab": {}
      },
      "source": [
        "# Get a data loader with no augmentation\n",
        "train_loader_fc1 = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST('../data', train=True, download=True,\n",
        "                   transform=transforms.Compose([\n",
        "                       transforms.ToTensor(),\n",
        "                       transforms.Normalize((0.1307,), (0.3081,))\n",
        "                   ])),\n",
        "    batch_size=optimizers[-1].batch_size, shuffle=False, **optimizers[-1].kwargs)\n",
        "\n",
        "train_fc1, train_target=get_fc1_features(optimizers[-1], train_loader_fc1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "dlSlgqR40afs",
        "colab": {}
      },
      "source": [
        "nbrs = NearestNeighbors(n_neighbors=16, algorithm='ball_tree').fit(train_fc1)\n",
        "distances, indices = nbrs.kneighbors(fc1[failure_indices[sorted_failures],:])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "TKmuqgu50afs",
        "colab": {}
      },
      "source": [
        "cnt = len(sorted_failures)\n",
        "\n",
        "failure_classes = output_all[failure_indices[sorted_failures]].argmax(dim=1, keepdim=True) # get the index of the max log-probability\n",
        "\n",
        "failure_img = tile_digits(failure_indices[sorted_failures], shape='Vertical')[0]\n",
        "failure_classification_img = tile_digits((-failure_classes.to(torch.device(\"cpu\")).numpy()-1).transpose()[0], shape='Vertical')[0]\n",
        "neigh_img = np.vstack([tile_digits(indices[idx,:], shape='Horizontal', train=True)[0] for idx in range(cnt)])\n",
        "\n",
        "imageio.imwrite(os.path.join(data_path, 'neighbors_train.png'), np.hstack([failure_img, failure_classification_img, neigh_img]))\n",
        "\n",
        "fig, ax=plt.subplots(figsize=(5,5), dpi=200)\n",
        "ax.axis('off')\n",
        "ax.imshow(np.hstack([failure_img, failure_classification_img, neigh_img]))\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "SW2GawFu0aft"
      },
      "source": [
        "## Plot Confusion Matrix\n",
        "The confusion matrix plotting function was copied from [Yassine Ghouzam's great notebook](https://www.kaggle.com/yassineghouzam/introduction-to-cnn-keras-0-997-top-6)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "w8FzGZDi0aft",
        "colab": {}
      },
      "source": [
        "def plot_confusion_matrix(cm, classes,\n",
        "                          normalize=False,\n",
        "                          title='Confusion matrix',\n",
        "                          cmap=plt.cm.Blues):\n",
        "    \"\"\"\n",
        "    This function prints and plots the confusion matrix.\n",
        "    Normalization can be applied by setting `normalize=True`.\n",
        "    \"\"\"\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.colorbar()\n",
        "    plt.title(title)\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=45)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "\n",
        "    thresh = cm.max() / 2.\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, cm[i, j],\n",
        "                 horizontalalignment=\"center\",\n",
        "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "C4rOYyMp0afu",
        "colab": {}
      },
      "source": [
        "pred_all = output_all.argmax(dim=1, keepdim=True) # get the index of the max log-probability\n",
        "            \n",
        "# compute the confusion matrix\n",
        "confusion_mtx = confusion_matrix(target_all.to(torch.device(\"cpu\")), pred_all.to(torch.device(\"cpu\"))) \n",
        "# plot the confusion matrix\n",
        "plot_confusion_matrix(confusion_mtx, classes = range(10)) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "WJDZrpgG0afv"
      },
      "source": [
        "## Look at weights/kernels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "O1durWeN0afv",
        "colab": {}
      },
      "source": [
        "def plot_kernels(conv_layer):\n",
        "    out_dim = conv_layer.size()[0]\n",
        "    in_dim = conv_layer.size()[1]\n",
        "    kw = conv_layer.size()[2]\n",
        "    kh = conv_layer.size()[3]\n",
        "\n",
        "    for row in range(0,out_dim):\n",
        "        fig, ax = plt.subplots(1, in_dim)\n",
        "        for col in range(0,in_dim):\n",
        "            if in_dim>1:\n",
        "                col_ax = ax[col]\n",
        "            else:\n",
        "                col_ax = ax\n",
        "            col_ax.imshow(conv_layer[row][col].numpy(), cmap='binary')\n",
        "            col_ax.axis('off')\n",
        "        plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "zU2J3VJE0afw",
        "colab": {}
      },
      "source": [
        "plot_kernels(optimizers[-1].model.state_dict()['conv1.weight'].permute(1,0,2,3).to(torch.device(\"cpu\")))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "WVCBLhJv0afy",
        "scrolled": false,
        "colab": {}
      },
      "source": [
        "plot_kernels(optimizers[-1].model.state_dict()['conv2.weight'].to(torch.device(\"cpu\")))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4i-sZ_BLrhSY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}